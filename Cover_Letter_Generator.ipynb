{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install -q -U langchain transformers bitsandbytes accelerate langchain-community langchain-core huggingface_hub"
      ],
      "metadata": {
        "id": "vbmy5FBl1cqn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wKtMYBinzDIR"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "\n",
        "import torch\n",
        "import os\n",
        "from langchain import PromptTemplate, HuggingFacePipeline\n",
        "from transformers import BitsAndBytesConfig, AutoModelForCausalLM, AutoTokenizer, GenerationConfig, pipeline\n",
        "\n",
        "hfapi = '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'\n",
        "model_name = \"HuggingFaceTB/SmolLM-1.7B-Instruct\"\n",
        "\n",
        "def init_model(model_name, hfapi):\n",
        "    MODEL_NAME = model_name\n",
        "    # Additional model options:\n",
        "    # MODEL_NAME =\"mistralai/Mistral-7B-Instruct-v0.2\"\n",
        "    # MODEL_NAME =\"meta-llama/Meta-Llama-3–8B\"\n",
        "    # MODEL_NAME =\"microsoft/Phi-3-mini-4k-instruct\"\n",
        "    # MODEL_NAME =\"microsoft/phi-1_5\"\n",
        "\n",
        "    # Initialize tokenizer\n",
        "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True, token=hfapi)\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    # Initialize language model\n",
        "    model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, torch_dtype=torch.float16,\n",
        "    trust_remote_code=True, device_map=\"auto\", token=hfapi)\n",
        "\n",
        "    return model, tokenizer\n",
        "\n",
        "def settings(model_name):\n",
        "    MODEL_NAME = model_name\n",
        "    generation_config = GenerationConfig.from_pretrained(MODEL_NAME)\n",
        "    generation_config.max_new_tokens = 1024\n",
        "    generation_config.temperature = 0.7\n",
        "    generation_config.top_p = 0\n",
        "    generation_config.do_sample = True\n",
        "\n",
        "    return generation_config\n",
        "\n",
        "def pipe_func(model_name, hfapi, settings):\n",
        "    model, tokenizer = init_model(model_name, hfapi)\n",
        "    generation_config = settings(model_name)\n",
        "    pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, return_full_text=True,\n",
        "    generation_config=generation_config)\n",
        "\n",
        "    return pipe"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "template = \"\"\"\n",
        "write a cover letter for a {topic} developer\n",
        "\"\"\"\n",
        "prompt = PromptTemplate(input_variables=[\"topic\"], template=template)\n",
        "topic = \"ai\"\n",
        "\n",
        "# Construct a Langchain Chain to connect the prompt template with the LLM\n",
        "chain = prompt | llm\n",
        "output = chain.invoke({\"topic\": topic})"
      ],
      "metadata": {
        "id": "4tLlAu03btse"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Solução TRADUZIDA para a resposta do Modelo.\n",
        "\n",
        "[Seu Nome]\n",
        "[Endereço]\n",
        "[Cidade, Estado CEP]\n",
        "[Endereço de E-mail]\n",
        "[Número de Telefone]\n",
        "\n",
        "[Data]\n",
        "\n",
        "[Nome do Destinatário]\n",
        "[Cargo do Destinatário]\n",
        "[Empresa do Destinatário]\n",
        "\n",
        "Prezado(a) [Nome do Destinatário],\n",
        "\n",
        "Estou escrevendo para expressar meu interesse na posição de [Título da Vaga] na [Nome da Empresa]. Possuo uma sólida formação em [habilidade ou experiência relevante], o que me torna um candidato ideal para esta posição.\n",
        "\n",
        "Em meu currículo, destaco minhas habilidades e experiências nas seguintes áreas:\n",
        "\n",
        "* [Habilidade 1]\n",
        "* [Habilidade 2]\n",
        "* [Habilidade 3]\n",
        "\n",
        "Além disso, demonstrei minha capacidade de trabalhar eficazmente em equipe e de comunicar de forma clara e concisa.\n",
        "\n",
        "Estou motivado a aprender e crescer na minha carreira, e acredito que esta posição proporcionará essa oportunidade.\n",
        "\n",
        "Agradeço a oportunidade de discutir minhas qualificações mais detalhadamente.\n",
        "\n",
        "Obrigado pelo seu tempo.\n",
        "\n",
        "Atenciosamente,\n",
        "\n",
        "[Seu Nome]\n",
        "\n",
        "[Seu Endereço]\n",
        "[Cidade, Estado CEP]\n",
        "\n",
        "[Endereço de E-mail]\n",
        "[Número de Telefone]"
      ],
      "metadata": {
        "id": "kqv12JK_5o9N"
      }
    }
  ]
}